{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356c03d-96a5-4e4b-aa50-4aa97b7ba69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# üß† 03 - GAN-based Artifact Removal\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook focuses on applying a trained GAN model to enhance CT images by removing motion and beam hardening artifacts.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# üì¶ Imports\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"from torchvision import transforms\\n\",\n",
    "    \"from torchvision.utils import make_grid\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"from PIL import Image\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\\n\",\n",
    "    \"\\n\",\n",
    "    \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üîÑ Load Data (Artifact + Clean Pair)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def load_image(path):\\n\",\n",
    "    \"    img = Image.open(path).convert('L')\\n\",\n",
    "    \"    return img\\n\",\n",
    "    \"\\n\",\n",
    "    \"transform = transforms.Compose([\\n\",\n",
    "    \"    transforms.Resize((256, 256)),\\n\",\n",
    "    \"    transforms.ToTensor()\\n\",\n",
    "    \"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"img_artifact = transform(load_image(\\\"../data/artifact/sample_ct_artifact_slice.png\\\")).unsqueeze(0).to(device)\\n\",\n",
    "    \"img_clean = transform(load_image(\\\"../data/raw/sample_ct_slice.png\\\")).unsqueeze(0).to(device)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üß¨ Load Pretrained GAN Generator (e.g., U-Net / pix2pix)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"class UNetGenerator(nn.Module):\\n\",\n",
    "    \"    def __init__(self):\\n\",\n",
    "    \"        super().__init__()\\n\",\n",
    "    \"        self.encoder = nn.Sequential(\\n\",\n",
    "    \"            nn.Conv2d(1, 64, 4, stride=2, padding=1),\\n\",\n",
    "    \"            nn.ReLU(),\\n\",\n",
    "    \"            nn.Conv2d(64, 128, 4, stride=2, padding=1),\\n\",\n",
    "    \"            nn.ReLU()\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        self.decoder = nn.Sequential(\\n\",\n",
    "    \"            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\\n\",\n",
    "    \"            nn.ReLU(),\\n\",\n",
    "    \"            nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1),\\n\",\n",
    "    \"            nn.Tanh()\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def forward(self, x):\\n\",\n",
    "    \"        x = self.encoder(x)\\n\",\n",
    "    \"        x = self.decoder(x)\\n\",\n",
    "    \"        return x\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load model\\n\",\n",
    "    \"model = UNetGenerator().to(device)\\n\",\n",
    "    \"model.load_state_dict(torch.load(\\\"../src/gan_artifact_removal/unet_generator.pth\\\", map_location=device))\\n\",\n",
    "    \"model.eval()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## ‚ú® Denoise via GAN Inference\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"with torch.no_grad():\\n\",\n",
    "    \"    output = model(img_artifact)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Convert tensors to images\\n\",\n",
    "    \"to_img = transforms.ToPILImage()\\n\",\n",
    "    \"artifact_img = to_img(img_artifact.squeeze().cpu())\\n\",\n",
    "    \"clean_img = to_img(img_clean.squeeze().cpu())\\n\",\n",
    "    \"output_img = to_img(output.squeeze().cpu())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üñºÔ∏è Visual Comparison\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"fig, axs = plt.subplots(1, 3, figsize=(12, 4))\\n\",\n",
    "    \"axs[0].imshow(artifact_img, cmap='gray'); axs[0].set_title('Input w/ Artifact')\\n\",\n",
    "    \"axs[1].imshow(output_img, cmap='gray'); axs[1].set_title('GAN Output')\\n\",\n",
    "    \"axs[2].imshow(clean_img, cmap='gray'); axs[2].set_title('Ground Truth')\\n\",\n",
    "    \"for ax in axs: ax.axis('off')\\n\",\n",
    "    \"plt.tight_layout(); plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## üìä Quantitative Evaluation\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"output_np = np.array(output_img)\\n\",\n",
    "    \"clean_np = np.array(clean_img)\\n\",\n",
    "    \"\\n\",\n",
    "    \"gan_psnr = psnr(clean_np, output_np)\\n\",\n",
    "    \"gan_ssim = ssim(clean_np, output_np)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"GAN Reconstruction - PSNR: {:.2f}, SSIM: {:.4f}\\\".format(gan_psnr, gan_ssim))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## ‚úÖ Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"- GAN significantly improves visual quality over artifact-affected CT.\\n\",\n",
    "    \"- Quantitative metrics confirm enhancement.\\n\",\n",
    "    \"- Output can feed into registration/VR pipelines.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Next**: Move to `04_registration_pipeline.ipynb` to align time-series or 4D CT frames.\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"name\": \"python\",\n",
    "   \"version\": \"\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
